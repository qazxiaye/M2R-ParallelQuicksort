# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages
#+TITLE:       Laboratory Notebook for a Multi-Threaded Version of Quicksort
#+AUTHOR:      Arnaud Legrand
#+LANGUAGE:    en
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n)

* Project Overview
This project aims at providing an efficient multi-threaded
implementation of the QuickSort algorithm on multi-core machines. This
document contains my attempts to evaluate the performance of an
implementation of such code.
* General Organization
** src/
This directory comprises the parallel implementation and a standard
Makefile to compile it.
** data/
This is where raw experimental data should go. Each directory entry
comprises a set of experiments and the directory name is based on the
machine name and on the date. For example:
#+begin_src sh :results output :exports both 
echo mkdir data/`hostname`_`date +%F`
#+end_src

#+RESULTS:
: mkdir data/sama_2014-10-13

* Typical usage
** Compilation
A simple makefile with various compilation options is provided in the
src/ directory. Compilation is thus done by running the following command:
#+begin_src sh :results output :exports both 
make -C src/
#+end_src

#+RESULTS:
: make: Entering directory '/home/alegrand/Work/Documents/Enseignements/M2R_Eval_Perf_13/M2R-ParallelQuicksort/src'
: cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread -lrt -std=c99  -c -o parallelQuicksort.o parallelQuicksort.c
: cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread -lrt -std=c99  parallelQuicksort.o  -o parallelQuicksort 
: make: Leaving directory '/home/alegrand/Work/Documents/Enseignements/M2R_Eval_Perf_13/M2R-ParallelQuicksort/src'

** Running the code
The code is quite simple at the moment and can be run in the following way:
#+begin_src
./src/parallelQuicksort [1000000]
#+end_src
When run, the code executes initializes an array of the size given in
argument (1000000 by default) with random integer values and sorts it
using:
1. a custom sequential implementation;
2. a custom parallel implementation;
3. the libc qsort function.
Times are reported in seconds.
* Experimental Reports
** 2014-10-13
*** Initial code design
- I obtained an initial implementation from
  http://sc12.supercomputing.org/hpceducator/PythonForParallelism/codes/parallelQuicksort.c.
  According to the header, the original author is Joshua Stough from
  Washington and Lee University. I hope he will not mind that I reuse
  this piece of code for educational purposes.
- Here is a typical first execution on my laptop (an Intel(R) Core(TM)
  i7 running a Debian with Linux 3.14.15):
  #+begin_src sh :results output :exports both 
    ./src/quicksort
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.231571 sec.
  : at loc 506315, 5.068226e-01 < 5.068269e-01 
  : Oops, lyst did not get sorted by parallelQuicksort.
  : Parallel quicksort took: 0.161259 sec.
  : Built-in qsort took: 0.241568 sec.

  Sweet, in my first attempt, it looks like this parallel version is
  indeed running faster than then sequential one. I have to say this
  warning message is stressing me a bit though.
- On smaller instances, the code would segfault. So I reindented the
  code and thanks to valgrind and gdb, I could find what was wrong. I
  also renamed the file so that compiling is more convenient. This
  fixed the previous warning message so now everything seems fine:
  #+begin_src sh :results output :exports both 
    ./src/parallelQuicksort
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.239347 sec.
  : Parallel quicksort took: 0.176365 sec.
  : Built-in quicksort took: 0.244716 sec.

*** First series of experiments
Let's try to see how the three algorithms behave when changing the 
array size. Since one measurement is not enough, I run the code 5
times in a row.
#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 1000000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src
I obtained the following [[file:data/sama_2014-10-13/measurements_03:47.txt][output]].

*** A simple plot with R
Here is a simple script to parse the results:
#+begin_src perl :results output raw :exports both :tangle scripts/csv_quicksort_extractor.pl
  use strict;

  my($line);
  my($size);

  print "Size, Type, Time\n" ;
  while($line=<>) {
      chomp $line;
      if($line =~/^Size: ([\d\.]*)$/) {
          $size = $1;
          next;
      } 
      if($line =~/^(.*) quicksort.*: ([\d\.]*) sec.$/) {
          print "$size, \"$1\", $2\n" ;
          next;
      } 
  }
#+end_src

I can then simply parse my data with the following command:

#+begin_src sh :results output :exports both 
perl scripts/csv_quicksort_extractor.pl < data/sama_2014-10-13/measurements_03\:47.txt > data/sama_2014-10-13/measurements_03\:47.csv
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file data/sama_2014-10-13/measurements_03:47.png :exports both :width 600 :height 400 :session
  df <- read.csv("data/sama_2014-10-13/measurements_03:47.csv",header=T)
  plot(df$Size,df$Time,col=c("red","blue","green")[df$Type])
#+end_src

#+RESULTS:
[[file:data/sama_2014-10-13/measurements_03:47.png]]

Well, this is not particularly nice and some may not know/like R.
*** A simple plot with gnuplot
So let's try to parse in an other way and use gnuplot:

#+begin_src perl :results output raw :exports both :tangle scripts/csv_quicksort_extractor2.pl
  use strict;

  my($line);
  my($size);
  my($seq,$par,$libc);
  print "Size, Seq, Par, Libc\n" ;
  while($line=<>) {
      chomp $line;
      if($line =~/^Size: ([\d\.]*)$/) {
          $size = $1;
          next;
      } 
      if($line =~/^Sequential quicksort.*: ([\d\.]*) sec.$/) {
          $seq=$1; next;
      } 
      if($line =~/^Parallel quicksort.*: ([\d\.]*) sec.$/) {
          $par=$1; next;
      } 
      if($line =~/^Built-in quicksort.*: ([\d\.]*) sec.$/) {
          $libc=$1; 
          print "$size, $seq, $pqr, $libc\n";
          next;
      }
  }
#+end_src

#+begin_src sh :results output raw :exports both 
  FILENAME="data/sama_2014-10-13/measurements_03:47"
  perl scripts/csv_quicksort_extractor2.pl < "$FILENAME.txt" > "${FILENAME}_wide.csv"
  echo "
    set terminal png size 600,400 
    set output '${FILENAME}_wide.png'
    set datafile separator ','
    set key autotitle columnhead
    plot '${FILENAME}_wide.csv' using 1:2 with linespoints, '' using 1:3 with linespoints, '' using 1:4 with linespoints
  " | gnuplot
  echo [[file:${FILENAME}_wide.png]]
#+end_src

#+RESULTS:
[[file:data/sama_2014-10-13/measurements_03:47_wide.png]]

Well, I'm not sure it is nicer but I have lines. A first crude
analysis seems to reveal the the parallel version is worth it for
arrays larger than 400000.

** 29/10/2014
*** first execution on my laptop
#+begin_src sh :results output :exports both 
    ./src/parallelQuicksort
#+end_src

#+results:
: Sequential quicksort took: 0.237812 sec.
: Parallel quicksort took: 0.194654 sec.
: Built-in quicksort took: 0.243592 sec.

** 30/10/2014
*** first execution on g5k
1. ssh xye@access.grid5000.fr
2. ssh grenoble
3. oarsub -I -l host=2/core=1 -e ~/my_job_key

** 02/11/2014
*** files
**** [[file:data/distribution.sh][data/distribution.sh]]
shell to execute parallelQuicksort 1000 times for each array size
**** data/ubuntu_2014-11-02/distribution/distribution_${size}.txt
original data generated by [[file:data/distribution.sh][distribution.sh]]
**** [[file:data/ubuntu_2014-11-02/distribution/getData.tcl][data/ubuntu_2014-11-02/distribution/getData.tcl]]
script to extract data from distribution_${size}.txt
**** data/ubuntu_2014-11-02/distribution/${size}_${type}
data extracted by [[file:data/ubuntu_2014-11-02/distribution/getData.tcl][getData.tcl]]


**** [[file:data/extractor_g5k.tcl][data/extractor_g5k.tcl]]
file to call csv_quicksort_extractor2.pl to extract all data got from g5k
**** data/g5k/host${n}core${n}.txt
data extracted by [[file:data/extractor_g5k.tcl][extractor_g5k.tcl]]
**** [[file:data/g5k/plot.sh][data/g5k/plot.sh]]
shell to generate plot from extracted data
**** [[file:data/g5k/tcl_plot.tcl][data/g5k/tcl_plot.tcl]]
file to call [[file:data/g5k/plot.sh][plot.sh] to generate all the png and save in [[file:data/g5k/png][data/g5k/png]]
**** [[file:data/g5k/png][data/g5k/png]]
graphs generated with [[file:data/g5k/tcl_plot.tcl][tcl_plot.tcl]]
**** [[file:data/g5k/plot png][data/g5k/plot png]]
graphs generated with java
**** [[file:data/g5k/data][data/g5k/data]]
original data got from g5k

*** analysis
[[file:analysis.pdf][analysis.pdf]]


** 07/12/2014
*** Environement
**** Host
***** CPU
processor	: 0
model name	: Intel(R) Core(TM) i5-3210M CPU @ 2.50GHz
siblings	: 4
cpu cores	: 2

***** Cache
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              3072K

***** RAM
#+begin_src sh :results output :exports both 
    cat /proc/meminfo
#+end_src

#+results:
#+begin_example
MemTotal:        3951816 kB
MemFree:          174548 kB
Buffers:          889516 kB
Cached:          1563412 kB
SwapCached:            0 kB
Active:          1815368 kB
Inactive:        1760508 kB
Active(anon):     936180 kB
Inactive(anon):   500508 kB
Active(file):     879188 kB
Inactive(file):  1260000 kB
Unevictable:        1300 kB
Mlocked:              16 kB
SwapTotal:        262140 kB
SwapFree:         262140 kB
Dirty:               132 kB
Writeback:             0 kB
AnonPages:       1124248 kB
Mapped:           190252 kB
Shmem:            312464 kB
Slab:             110396 kB
SReclaimable:      79224 kB
SUnreclaim:        31172 kB
KernelStack:        3696 kB
PageTables:        30044 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     2238048 kB
Committed_AS:    3555328 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      444420 kB
VmallocChunk:   34359290460 kB
HardwareCorrupted:     0 kB
AnonHugePages:         0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       95404 kB
DirectMap2M:     4003840 kB
#+end_example

**** OS
Kernel Version :
#+begin_src sh :results output :exports both 
    uname -r
#+end_src

#+results:
: 3.11.0-15-generic

**** Compiler
#+begin_src sh :results output :exports both 
    gcc --version
#+end_src

#+results:
: gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3
: Copyright (C) 2011 Free Software Foundation, Inc.
: This is free software; see the source for copying conditions.  There is NO
: warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

*** Random Ordered Experiment
I have done a Random Ordered Experiment for n={10,100,1000,10000,100000} to get the distribution of the execution time on my computer. But I still get 2 peaks as the following figure.

[[file:data/ubuntu_2014-12-07/distribution_ran_order/png/10000_seq.png]]
[[file:data/ubuntu_2014-12-07/distribution_ran_order/png/10000_para.png]]
[[file:data/ubuntu_2014-12-07/distribution_ran_order/png/10000_libc.png]]

It's obvious they don't follow the normal distribution. In the following analysis, I will compare the Confidence Interval and average value.

*** time vs n
[[file:data/ubuntu_2014-12-07/distribution_ran_order/time_vs_n.png]]

As in the graph, the gray part shows the confidence interval, the line shows the average value. Obviously, we don't get a good performance with the parallel execution.

*** gcc option
**** O2
When I modify gcc option from -O0 to -O2, for the same experiment, I get the following result.

[[file:data/ubuntu_2014-12-07/distribution_ran_order/time_vs_n.png]]

There's no real difference found compared with the previous one. But for "seq" and "libc", at size 100000, we get a better performance than the previous version.

**** O3
[[file:data/ubuntu_2014-12-07/distribution_ran_order/time_vs_n.png]]

Similar result with O3, parallel performance is even worse. 

*** thread level
It seems to be too many threads created, I modified the source code, assign a smaller value to top thread level.
**** tlevel 5
When top thread level is 5, I get the following result:

[[file:data/ubuntu_2014-12-07/tlevel5/time_vs_n.png]]

In this case, the confidence interval of each line is quite narrow. we cannot see it in this graph. The performance is much better.

**** time vs p
For other value of tlevel the results are as the following :

tlevel = 3
[[file:data/ubuntu_2014-12-07/tlevel3/time_vs_n.png]]
Compared with the result when tlevel = 5, when n is smaller, we get a good performance in this case with less cost of merge and thread manage.


tlevel = 0
[[file:data/ubuntu_2014-12-07/tlevel0/time_vs_n.png]]
When tlevel = 0, there will be only one thread for parallel execution. With the cost of thread manage, it's quite usual that it performes worse than the sequencial versions.


TODO : time vs p, speedup, efficiency


